---
title: "Modelling effectiveness of exercise from accelerometer data"
author: "Kevin Ho"
date: "18 October 2016"
output: html_document
---
```{r, echo=FALSE, include=FALSE}
#knitr setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
#Set Workspace 
setwd("C:/Users/Kevin/Documents/Learning/Machine Learning/John Hopkins ML - Assignment")
```
#Executive Summary
The aim of the project is to create a model from the Human Activity Recognition Dataset to predict the manner in which 6 participants did exercise and then measure its performance with a validation set of 20 observations. The model had a prediction accuracy of 99.39% over 15699 observations.

#Overview
One thing that people regularly do is quantify how *much* of a particular activity they do, but rarely quantify *how well they do it*. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

This study aimed to predict the manner in which the participants did the exercise how 

#Setup
## Load Packages
```{r, echo = TRUE, warning=FALSE, error=FALSE, message=FALSE}
require(ggplot2)
require(caret)
require(dplyr)
require(randomForest)
```

##Divide the data
The training set is divided to a test and training the set. While the test data will be labelled as the predication data.

Predictor variables with more than 5% errors and non-sensor data will be removed from the dataset.
```{r}
errorRateThreshold <- .05

training <- read.csv("./pml-training.csv", na.strings=c("NA","#DIV/0!",""))
predictionData <- read.csv("./pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

#Evaluate the error rate per variable
varErrorRate <- apply(is.na(training),2,sum)/nrow(training)

#Removing variables below threshold
training <- training[varErrorRate < errorRateThreshold]
predictionData <- predictionData[varErrorRate < errorRateThreshold]

#First 7 columns removed as they are not predictors.
training <- training[,-c(1:7)]
predictionData <- predictionData[,-c(1:7)]
```

And split training into a test and training set

```{r}
set.seed(19836)
inTrain <- createDataPartition(y=training$classe, p= 0.8, list = FALSE)
test <- training[-inTrain,]
training <- training[inTrain,]
```

#Model Selection
Lets test our predictor variables for normality. We'll use the test sample as the training sample is too large for the shapiro-wilk test
```{r}
a <- vector('numeric')
    for (i in 1:length(names(test))) {
        if(is.numeric((test[,i]))) {
            a <- c(a,shapiro.test(test[,i])$p.value)
            shapiro.test(test[,i])$p.value
        }
    }

summary(a)
```

This is a good sign of noromality. And the following shows that all are variables are now of numeric type.

```{r}
table(sapply(training[1,],class))
```

#Modelling

For ease of this analysis we'll use random forests so that the variables don't have to be transformed before building the model.

```{r}
model <- train(classe~., training, method="rf")

prediction <- predict(model,newdata=test)

ConMat <- confusionMatrix(prediction,test$classe)
```

With an accuracy of `r round(ConMat$overall[1],5)*100`%. Which lies within the 95% confidence interval. The model appears to be a good fit for the data.

#Prediction of the validation data set
Lets make our predictions and output it to a file
```{r}
predictions <- predict(model,newdata = predictionData)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictions)
```


#Notes

##Environment 
```{r}
R.version.string
Sys.info()[1:3]
```
##References
1. [Groupware@LES Data from HAR](http://groupware.les.inf.puc-rio.br/har)
2. [Diogo Auerlio's Machine learning assignment](https://rstudio-pubs-static.s3.amazonaws.com/29426_041c5ccb9a6a4bedb204e33144bb0ad4.html)

